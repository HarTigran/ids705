{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Supervised Learning: model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tigran Harutyunyan\n",
    "Netid: Th314\n",
    "\n",
    "*Names of students you worked with on this assignment*: LIST HERE IF APPLICABLE (delete if not)\n",
    "\n",
    "Note: this assignment falls under collaboration Mode 2: Individual Assignment â€“ Collaboration Permitted. Please refer to the syllabus for additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions for all assignments can be found [here](https://github.com/kylebradbury/ids705/blob/master/assignments/_Assignment%20Instructions.ipynb), and is also linked to from the [course syllabus](https://kylebradbury.github.io/ids705/index.html).\n",
    "\n",
    "Total points in the assignment add up to 90; an additional 10 points are allocated to presentation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives:\n",
    "This assignment will provide structured practice to help enable you to...\n",
    "1. Understand the primary workflow in machine learning: (1) identifying a hypothesis function set of models, (2) determining a loss/cost/error/objective function to minimize, and (3) minimizing that function through gradient descent\n",
    "2. Understand the inner workings of logistic regression and how linear models for classification can be developed.\n",
    "3. Gain practice in implementing machine learning algorithms from the most basic building blocks to understand the math and programming behind them to achieve practical proficiency with the techniques\n",
    "4. Implement batch gradient descent and become familiar with how that technique is used and its dependence on the choice of learning rate\n",
    "5. Evaluate supervised learning algorithm performance through ROC curves and using cross validation\n",
    "6. Apply regularization to linear models to improve model generalization performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "## Classification using logistic regression: build it from the ground up\n",
    "**[60 points]**\n",
    "\n",
    "This exercise will walk you through the full life-cycle of a supervised machine learning classification problem. Classification problem consists of two features/predictors (e.g. petal width and petal length) and your goal is to predict one of two possible classes (class 0 or class 1). You will build, train, and evaluate the performance of a logistic regression classifier on the data provided. Before you begin any modeling, you'll load and explore your data in Part I to familiarize yourself with it - and check for any missing or erroneous data. Then, in Part II, we will review an appropriate hypothesis set of functions to fit to the data: in this case, logistic regression. In Part III, we will derive an appropriate cost function for the data (spoiler alert: it's cross-entropy) as well as the gradient descent update equation that will allow you to optimize that cost function to identify the parameters that minimize the cost for the training data. In Part IV, all the pieces come together and you will implement your logistic regression model class including methods for fitting the data using gradient descent. Using that model you'll test it out and plot learning curves to verify the model learns as you train it and to identify and appropriate learning rate hyperparameter. Lastly, in Part V you will apply the model you designed, implemented, and verified to your actual data and evaluate and visualize its generalization performance as compared to a KNN algorithm. **When complete, you will have accomplished learning objectives 1-5 above!**\n",
    "\n",
    "### I. Load, prepare, and plot your data\n",
    "You are given some data for which you are tasked with constructing a classifier. The first step when facing any machine learning project: look at your data!\n",
    "\n",
    "**(a)** Load the data. \n",
    "- In the data folder in the same directory of this notebook, you'll find the data in `A3_Q1_data.csv`. This file contains the binary class labels, $y$, and the features $x_1$ and $x_2$.\n",
    "- Divide your data into a training and testing set where the test set accounts for 30 percent of the data and the training set the remaining 70 percent.  \n",
    "- Plot the training data by class. \n",
    "- Comment on the data: do the data appear separable? May logistic regression be a good choice for these data? Why or why not?\n",
    "\n",
    "**(b)** Do the data require any preprocessing due to missing values, scale differences (e.g. different ranges of values), etc.? If so, how did you handle these issues?\n",
    "\n",
    "Next, we walk through our key steps for model fitting: choose a hypothesis set of models to train (in this case, logistic regression); identify a cost function to measure the model fit to our training data; optimize model parameters to minimize cost (in this case using gradient descent). Once we've completed model fitting, we will evaluate the performance of our model and compare performance to another approach (a KNN classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Stating the hypothesis set of models to evaluate (we'll use logistic regression)\n",
    "\n",
    "Given that our data consists of two features, our logistic regression problem will be applied to a two-dimensional feature space. Recall that our logistic regression model is:\n",
    "\n",
    "$$f(\\mathbf{x}_i,\\mathbf{w})=\\sigma(\\mathbf{w}^{\\top} \\mathbf{x}_i)$$\n",
    "\n",
    "where the sigmoid function is defined as $\\sigma(x) = \\dfrac{e^x}{1+e^{x}}= \\dfrac{1}{1+e^{-x}}$. Also, since this is a two-dimensional problem, we define $\\mathbf{w}^{\\top} \\mathbf{x}_i = w_0 x_{i,0} + w_1 x_{i,1} + w_2 x_{i,2}$ and here, $\\mathbf{x}_i=[x_{i,0}, x_{i,1}, x_{i,2}]^{\\top}$, and $x_{i,0} \\triangleq 1$\n",
    "\n",
    "Remember from class that we interpret our logistic regression classifier output (or confidence score) as the conditional probability that the target variable for a given sample $y_i$ is from class \"1\", given the observed features, $\\mathbf{x}_i$. For one sample, $(y_i, \\mathbf{x}_i)$, this is given as:\n",
    "\n",
    "$$P(Y=1|X=\\mathbf{x}_i) = f(\\mathbf{x}_i,\\mathbf{w})=\\sigma(\\mathbf{w}^{\\top} \\mathbf{x}_i)$$\n",
    "\n",
    "In the context of maximizing the likelihood of our parameters given the data, we define this to be the likelihood function $L(\\mathbf{w}|y_i,\\mathbf{x}_i)$, corresponding to one sample observation from the training dataset.\n",
    "\n",
    "*Aside: the careful reader will recognize this expression looks different from when we talk about the likelihood of our data given the true class label, typically expressed as $P(x|y)$, or the posterior probability of a class label given our data, typically expressed as $P(y|x)$. In the context of training a logistic regression model, the likelihood we are interested in is the likelihood function of our logistic regression **parameters**, $\\mathbf{w}$. It's our goal to use this to choose the parameters to maximize the likelihood function.*\n",
    "\n",
    "**No output is required for this section - just read and use this information in the later sections.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Find the cost function that we can use to choose the model parameters, $\\mathbf{w}$, that best fit the training data.\n",
    "\n",
    "**(c)** What is the likelihood function that corresponds to all the $N$ samples in our training dataset that we will wish to maximize? Unlike the likelihood function written above which gives the likelihood function for a *single training data pair* $(y_i, \\mathbf{x}_i)$, this question asks for the likelihood function for the *entire training dataset* $\\{(y_1, \\mathbf{x}_1), (y_2, \\mathbf{x}_2), ..., (y_N, \\mathbf{x}_N)\\}$. \n",
    "\n",
    "**(d)** Since a logarithm is a monotonic function, maximizing the $f(x)$ is equivalent to maximizing $\\ln [f(x)]$. Express the likelihood from the last question as a cost function of the model parameters, $C(\\mathbf{w})$; that is the negative of the logarithm of the likelihood.\n",
    "\n",
    "**(e)** Calculate the gradient of the cost function with respect to the model parameters $\\nabla_{\\mathbf{w}}C(\\mathbf{w})$. Express this in terms of the partial derivatives of the cost function with respect to each of the parameters, e.g. $\\nabla_{\\mathbf{w}}C(\\mathbf{w}) = \\left[\\dfrac{\\partial C}{\\partial w_0}, \\dfrac{\\partial C}{\\partial w_1}, \\dfrac{\\partial C}{\\partial w_2}\\right]$. \n",
    "\n",
    "To simplify notation, please use $\\mathbf{w}^{\\top}\\mathbf{x}$ instead of writing out $w_0 x_{i,0} + w_1 x_{i,1} + w_2 x_{i,2}$ when it appears each time (where $x_{i,0} = 1$ for all $i$). You are also welcome to use $\\sigma()$ to represent the sigmoid function. Lastly, this will be a function the features, $x_{i,j}$ (with the first index in the subscript representing the observation and the second the feature; targets, $y_i$; and the logistic regression model parameters, $w_j$.\n",
    "\n",
    "**(f)** Write out the gradient descent update equation. This should clearly express how to update each weight from one step in gradient descent $w_j^{(k)}$ to the next $w_j^{(k+1)}$.  There should be one equation for each model logistic regression model parameter (or you can represent it in vectorized form). Assume that $\\eta$ represents the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Implement gradient descent and your logistic regression algorithm\n",
    "\n",
    "**(g)** Implement your logistic regression model. \n",
    "- You are provided with a template, below, for a class with key methods to help with your model development. It is modeled on the Scikit-Learn convention. For this, you only need to create a version of logistic regression for the case of two feature variables (i.e. two predictors).\n",
    "- Create a method called `sigmoid` that calculates the sigmoid function\n",
    "- Create a method called `cost` that computes the cost function $C(\\mathbf{w})$ for a given dataset and corresponding class labels. This should be the **average cost** (make sure your total cost is divided by your number of samples in the dataset).\n",
    "- Create a method called `gradient_descent` to run **one step** of gradient descent on your training data. We'll refer to this as \"batch\" gradient descent since it takes into account the gradient based on all our data at each iteration of the algorithm. \n",
    "- Create a method called `fit` that fits the model to the data (i.e. sets the model parameters to minimize cost) using your `gradient_descent` method. In doing this we'll need to make some assumptions about the following:\n",
    "    - Weight initialization. What should you initialize the model parameters to? For this, randomly initialize the weights to a different values between 0 and 1.\n",
    "    - Learning rate. How slow/fast should the algorithm step towards the minimum? This you will vary in a later part of this problem.\n",
    "    - Stopping criteria. When should the algorithm be finished searching for the optimum? There are two stopping criteria: small changes in the gradient descent step size and a maximum number of iterations. The first is whether there was a sufficiently small change in the gradient; this is evaluated as whether the magnitude of the step that the gradient descent algorithm takes changes by less than $10^{-6}$ between iterations. Since we have a weight vector, we can compute the change in the weight by evaluating the $L_2$ norm (Euclidean norm) of the change in the vector between iterations. From our gradient descent update equation we know that mathematically this is $||-\\eta\\nabla_{\\mathbf{w}}C(\\mathbf{w})||$. The second criterion is met if a maximum number of iterations has been reach (5,000 in this case, to prevent infinite loops from poor choices of learning rates).\n",
    "    - Design your approach so that at each step in the gradient descent algorithm you evaluate the cost function for both the training and the test data for each new value for the model weights. You should be able to plot cost vs gradient descent iteration for both the training and the test data. This will allow you to plot \"learning curves\" that can be informative for how the model training process is proceeding.\n",
    "- Create a method called `predict_proba` that predicts confidence scores (that can be thresholded into the predictions of the `predict` method.\n",
    "- Create a method called `predict` that makes predictions based on the trained model, selecting the most probable class, given the data, as the prediction, that is class that yields the larger $P(y|\\mathbf{x})$.\n",
    "- (Optional, but recommended) Create a method called `learning_curve` that produces the cost function values that correspond to each step from a previously run gradient descent operation.\n",
    "- (Optional, but recommended) Create a method called `prepare_x` which appends a column of ones as the first feature of the dataset $\\mathbf{X}$ to account for the bias term ($x_{i,1}=1$).\n",
    "\n",
    "This structure is strongly encouraged; however, you're welcome to adjust this to your needs (adding helper methods, modifying parameters, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression class\n",
    "class Logistic_regression:\n",
    "    # Class constructor\n",
    "    def __init__(self):\n",
    "        self.w = None     # logistic regression weights\n",
    "        self.saved_w = [] # Since this is a small problem, we can save the weights\n",
    "                          #  at each iteration of gradient descent to build our \n",
    "                          #  learning curves\n",
    "        # returns nothing\n",
    "        pass\n",
    "    \n",
    "    # Method for calculating the sigmoid function of w^T X for an input set of weights\n",
    "    def sigmoid(self, X, w):\n",
    "        # returns the value of the sigmoid\n",
    "        pass\n",
    "    \n",
    "    # Cost function for an input set of weights\n",
    "    def cost(self, X, y, w):\n",
    "        # returns the average cross entropy cost\n",
    "        pass\n",
    "    \n",
    "    # Update the weights in an iteration of gradient descent\n",
    "    def gradient_descent(self, X, y, lr):\n",
    "        # returns s scalar of the magnitude of the Euclidean norm \n",
    "        #  of the change in the weights during one gradient descent step\n",
    "        pass\n",
    "    \n",
    "    # Fit the logistic regression model to the data through gradient descent\n",
    "    def fit(self, X, y, w_init, lr, delta_thresh=1e-6, max_iter=5000, verbose=False):\n",
    "        # Note the verbose flag enables you to print out the weights at each iteration \n",
    "        #  (optional - but may help with one of the questions)\n",
    "        \n",
    "        # returns nothing\n",
    "        pass\n",
    "    \n",
    "    # Use the trained model to predict the confidence scores (prob of positive class in this case)\n",
    "    def predict_proba(self, X):\n",
    "        # returns the confidence score for the each sample\n",
    "        pass\n",
    "    \n",
    "    # Use the trained model to make binary predictions\n",
    "    def predict(self, X, thresh=0.5):\n",
    "        # returns a binary prediction for each sample\n",
    "        pass\n",
    "    \n",
    "    # Stores the learning curves from saved weights from gradient descent\n",
    "    def learning_curve(self, X, y):\n",
    "        # returns the value of the cost function from each step in gradient descent\n",
    "        #  from the last model fitting process\n",
    "        pass\n",
    "    \n",
    "    # Appends a column of ones as the first feature to account for the bias term\n",
    "    def prepare_x(self, X):\n",
    "        # returns the X with a new feature of all ones (a column that is the new column 0)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h)** Choose a learning rate and fit your model. Learning curves are a plot of metrics of model performance evaluated through the process of model training to provide insight about how model training is proceeding. Show the learning curves for the gradient descent process for learning rates of $\\{10^{-2}, 10^{-4}, 10^{-6}\\}$. For each learning rate plot the learning curves by plotting **both the training and test data average cost** as a function of each iteration of gradient descent. You should run the model fitting process until it completes (up to 5,000 iterations of gradient descent). Each of the 6 resulting curves (train and test average cost for each learning rate) should be plotted on the same set of axes for direct comparison. *Note: make sure you're using average cost per sample, not total cost*\n",
    "- Try running this process for a really big learning rate for this problem: $10^0$. Look at the weights that the fitting process generates over the first 50 iterations and how they change. You may simply print these first 50 iterations as output or plot them. What happens and why?\n",
    "- What is the impact that the different values of learning has on the speed of the process and the results? \n",
    "- Of the options explored, what learning rate do you prefer and why?\n",
    "- Use your chosen learning rate for the remainder of this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Evaluate your model performance through cross validation\n",
    "\n",
    "**(i)** Test the performance of your trained classifier using K-folds cross validation resampling technique. The scikit-learn package [StratifiedKFolds](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold) may be helpful. \n",
    "- Train your logistic regression model and a K-Nearest Neighbor classification model with $k=7$ nearest neighbors.\n",
    "- Using the trained models, make two plots corresponding to each model (logistic regression and KNN): one with the training data, and one for the test data. On each plot, include the decision boundary resulting from your trained classifier.\n",
    "- Produce a Receiver Operating Characteristic curve (ROC curve) that represents the performance from cross validated performance evaluation for each classifier (your logistic regression model and the KNN model, with $k=7$ nearest neighbors). For the cross validation, use $k=10$ folds. \n",
    "  - Plot these curves on the same set of axes to compare them\n",
    "  - On the ROC curve plot, also include the chance diagonal for reference (this represents the performance of the worst possible classifier). This is represented as a line from $(0,0)$ to $(1,1)$.\n",
    "  - Calculate the Area Under the Curve for each model and include this measure in the legend of the ROC plot.\n",
    "- Comment on the following:\n",
    "  - What is the purpose of using cross validation for this problem?\n",
    "  - How do the models compare in terms of performance (both ROC curves and decision boundaries) and which model (logistic regression or KNN) would you select to use on previously unseen data for this problem and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "I. (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt('data/A3_Q1_data.csv', delimiter=',', skiprows=1, usecols=[0,1])\n",
    "y = np.loadtxt('data/A3_Q1_data.csv', delimiter=',', skiprows=1, usecols=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split(X,y, test_size= 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot scatterplot of dataset\n",
    "plt.scatter(X_train[:, 0][y_train == 1], X_train[:, 1][y_train == 1], s=10, c=\"orange\", label=\"Class 1\")\n",
    "plt.scatter(X_train[:, 0][y_train == 0], X_train[:, 1][y_train == 0], s=10, c=\"purple\", label=\"Class 0\")\n",
    "plt.title(\"Train Dataset\")\n",
    "plt.xlabel(\"x_1\")\n",
    "plt.ylabel(\"x_2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are some observations that are difficult to distinguish the class, the overall data appears to be separable, with class one at the bottom left and class zero at the top right. Since the problem is a binary classification and the decision boundary looks linear, applying logistic regression is a good choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)\n",
    "- Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of missing values in X is:  0\n",
      "The number of missing values in y is:  0\n"
     ]
    }
   ],
   "source": [
    "print('The number of missing values in X is: ',np.isnan(X_train).sum())\n",
    "print('The number of missing values in y is: ',np.isnan(y_train).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check for scale differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for bouth predictores ranging from -3.21 to 3.9, hence no scale differances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_1</th>\n",
       "      <th>X_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>140.000000</td>\n",
       "      <td>140.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.013655</td>\n",
       "      <td>-0.476073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.395859</td>\n",
       "      <td>1.180769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.210005</td>\n",
       "      <td>-3.193456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.002628</td>\n",
       "      <td>-1.408136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.022974</td>\n",
       "      <td>-0.519640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.960201</td>\n",
       "      <td>0.357610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.862195</td>\n",
       "      <td>3.103541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              X_1         X_2\n",
       "count  140.000000  140.000000\n",
       "mean     0.013655   -0.476073\n",
       "std      1.395859    1.180769\n",
       "min     -3.210005   -3.193456\n",
       "25%     -1.002628   -1.408136\n",
       "50%      0.022974   -0.519640\n",
       "75%      0.960201    0.357610\n",
       "max      3.862195    3.103541"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to DataFrame to learn about predictors statistics.\n",
    "data = pd.DataFrame(X_train, columns= ['X_1','X_2'])\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions are bell-shaped and can be considered normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ploting distributions of predictores\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(6, 4))\n",
    "axes[0].hist(data['X_1'])\n",
    "axes[0].title.set_text('Histogram for X_1')\n",
    "axes[1].hist(data['X_2'])\n",
    "axes[1].title.set_text('Histogram for X_2')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)\n",
    "\n",
    "$$L(w|y_i,X_i) = \\prod_{i=1}^{N}\\sigma(w^Tx_i)^{y_i}[1 - \\sigma(w^Tx_i)^{1-y_i}]   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d)\n",
    "\n",
    "$$\\ln{L(w|y_i,X_i)} = \\sum_{i=1}^{N}({y_i\\ln{\\sigma(w^Tx_i)} + (1-y_i)\\ln[{1-\\sigma(w^Tx_i)}]}) $$\n",
    "\n",
    "$$C(w) = - \\ln{L(w|y_i,X_i)} = -[ \\sum_{i=1}^{N}({y_i\\ln{\\sigma(w^Tx_i)} + (1-y_i)\\ln[{1-\\sigma(w^Tx_i)}]})]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e)\n",
    "\n",
    "$\\nabla_{\\mathbf{w}}C(\\mathbf{w}) = \\left[\\dfrac{\\partial C}{\\partial w_0}, \\dfrac{\\partial C}{\\partial w_1}, \\dfrac{\\partial C}{\\partial w_2}\\right]$\n",
    "\n",
    "$\\dfrac{\\partial C}{\\partial w_0} = -\\sum_{i=1}^{N}[y_i(1 - \\sigma(w^Tx_i)) - (1-y_i)\\sigma(w^Tx_i)] =  \\sum_{i=1}^{N}[\\sigma(w^Tx_i) - y_i] $\n",
    "\n",
    "$\\dfrac{\\partial C}{\\partial w_1} = -\\sum_{i=1}^{N}x_{i,1}[y_i(1 - \\sigma(w^Tx_i)) - (1-y_i)\\sigma(w^Tx_i)] =  \\sum_{i=1}^{N}x_{i,1}[\\sigma(w^Tx_i) - y_i]$\n",
    "\n",
    "$\\dfrac{\\partial C}{\\partial w_2} = -\\sum_{i=1}^{N}x_{i,2}[y_i(1 - \\sigma(w^Tx_i)) - (1-y_i)\\sigma(w^Tx_i)] = \\sum_{i=1}^{N}x_{i,2}[\\sigma(w^Tx_i) - y_i]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w_0^{k+1} = w_0^k - \\eta*\\dfrac{\\partial C}{\\partial w_0^k}$\n",
    "\n",
    "$w_1^{k+1} = w_1^k - \\eta*\\dfrac{\\partial C}{\\partial w_1^k}$\n",
    "\n",
    "$w_2^{k+1} = w_2^k - \\eta*\\dfrac{\\partial C}{\\partial w_2^k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Logistic regression class\n",
    "class Logistic_regression:\n",
    "    # Class constructor\n",
    "    def __init__(self):\n",
    "        self.w = None     # logistic regression weights\n",
    "        self.saved_w = [] # Since this is a small problem, we can save the weights\n",
    "                          #  at each iteration of gradient descent to build our \n",
    "                          #  learning curves\n",
    "        # returns nothing\n",
    "        pass\n",
    "    \n",
    "    # Method for calculating the sigmoid function of w^T X for an input set of weights\n",
    "    def sigmoid(self, X, w):\n",
    "        self.X = X\n",
    "        self.w = w\n",
    "        sig = 1/(1+np.exp(-X_train@w.T))\n",
    "        # returns the value of the sigmoid\n",
    "        return sig\n",
    "    \n",
    "    # Cost function for an input set of weights\n",
    "    def cost(self, X, y, w):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.w = w\n",
    "        cost = -np.mean(y*np.log(self.sigmoid(X,w)) + (1-y)*np.log(1- self.sigmoid(X,w)))\n",
    "        # returns the average cross entropy cost\n",
    "        return cost\n",
    "    \n",
    "    # Update the weights in an iteration of gradient descent\n",
    "    def gradient_descent(self, X, y, lr):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.lr = lr\n",
    "        N = np.shape(self.X)[0]\n",
    "        s = (lr/N)*np.dot(X.T, (self.sigmoid(X,self.w) - y))\n",
    "        # returns s scalar of the magnitude of the Euclidean norm \n",
    "        #  of the change in the weights during one gradient descent step\n",
    "        return np.linalg.norm(s)\n",
    "    \n",
    "    # Fit the logistic regression model to the data through gradient descent\n",
    "    def fit(self, X, y, w_init, lr, delta_thresh=1e-6, max_iter=5000, verbose=False):\n",
    "        w_init = np.random.rand(3)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.lr = lr\n",
    "        self.w = w_init\n",
    "        for i in range(max_iter):\n",
    "            if self.gradient_descent(X, y, lr) > delta_thresh:\n",
    "                w_init = w_init - self.gradient_descent(X, y, lr)\n",
    "                self.saved_w.append(w_init)\n",
    "            else:\n",
    "                break\n",
    "        # Note the verbose flag enables you to print out the weights at each iteration \n",
    "        #  (optional - but may help with one of the questions)\n",
    "        \n",
    "        # returns nothing\n",
    "        pass\n",
    "    \n",
    "    # Use the trained model to predict the confidence scores (prob of positive class in this case)\n",
    "    def predict_proba(self, X):\n",
    "        # returns the confidence score for the each sample\n",
    "        return self.sigmoid(X,self.w)\n",
    "    \n",
    "    # Use the trained model to make binary predictions\n",
    "    def predict(self, X, thresh=0.5):\n",
    "        # returns a binary prediction for each sample\n",
    "        return np.array([1 if i > thresh else 0 for i in self.predict_proba(X)])\n",
    "    \n",
    "    # Stores the learning curves from saved weights from gradient descent\n",
    "    def learning_curve(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        cost_train = [self.cost(X, y, w) for w in self.saved_w]\n",
    "        # returns the value of the cost function from each step in gradient descent\n",
    "        #  from the last model fitting process\n",
    "        return cost_train\n",
    "    \n",
    "    # Appends a column of ones as the first feature to account for the bias term\n",
    "    def prepare_x(self, X):\n",
    "        # returns the X with a new feature of all ones (a column that is the new column 0)\n",
    "        self.X = X\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = Logistic_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.saved_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = k.prepare_x(X_train)\n",
    "w = np.random.rand(3).reshape((1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = k.fit(X_train,y_train,w,0.01,verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harut\\AppData\\Local\\Temp\\ipykernel_14688\\3341466286.py:27: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(y*np.log(self.sigmoid(X,w)) + (1-y)*np.log(1- self.sigmoid(X,w)))\n",
      "C:\\Users\\harut\\AppData\\Local\\Temp\\ipykernel_14688\\3341466286.py:27: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(y*np.log(self.sigmoid(X,w)) + (1-y)*np.log(1- self.sigmoid(X,w)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1722450852000996,\n",
       " 1.1599979819739892,\n",
       " 1.147822586919344,\n",
       " 1.1357202562554936,\n",
       " 1.123692363192936,\n",
       " 1.1117402972539652,\n",
       " 1.099865463509743,\n",
       " 1.0880692817285407,\n",
       " 1.0763531854296433,\n",
       " 1.0647186208372041,\n",
       " 1.0531670457281084,\n",
       " 1.0416999281677164,\n",
       " 1.0303187451271631,\n",
       " 1.0190249809757623,\n",
       " 1.0078201258419333,\n",
       " 0.9967056738360461,\n",
       " 0.985683121128584,\n",
       " 0.9747539638771674,\n",
       " 0.9639196959962136,\n",
       " 0.9531818067634086,\n",
       " 0.9425417782577398,\n",
       " 0.9320010826246259,\n",
       " 0.9215611791647167,\n",
       " 0.9112235112442715,\n",
       " 0.9009895030266557,\n",
       " 0.8908605560265154,\n",
       " 0.8808380454905779,\n",
       " 0.8709233166118134,\n",
       " 0.8611176805869046,\n",
       " 0.8514224105305854,\n",
       " 0.8418387372643874,\n",
       " 0.832367845001653,\n",
       " 0.8230108669552253,\n",
       " 0.8137688808989344,\n",
       " 0.804642904718698,\n",
       " 0.7956338919936204,\n",
       " 0.7867427276516749,\n",
       " 0.7779702237482325,\n",
       " 0.7693171154186138,\n",
       " 0.7607840570577754,\n",
       " 0.7523716187810441,\n",
       " 0.7440802832192505,\n",
       " 0.7359104426996295,\n",
       " 0.7278623968603363,\n",
       " 0.7199363507414019,\n",
       " 0.7121324133884711,\n",
       " 0.7044505969979031,\n",
       " 0.696890816622968,\n",
       " 0.6894528904512327,\n",
       " 0.6821365406531367,\n",
       " 0.6749413947915494,\n",
       " 0.6678669877721763,\n",
       " 0.6609127643053619,\n",
       " 0.6540780818414836,\n",
       " 0.647362213934983,\n",
       " 0.6407643539863518,\n",
       " 0.6342836193072309,\n",
       " 0.6279190554512044,\n",
       " 0.6216696407518967,\n",
       " 0.6155342910104876,\n",
       " 0.6095118642766295,\n",
       " 0.6036011656697587,\n",
       " 0.5978009521917681,\n",
       " 0.5921099374866677,\n",
       " 0.5865267965080345,\n",
       " 0.5810501700604529,\n",
       " 0.5756786691866509,\n",
       " 0.5704108793774108,\n",
       " 0.5652453645864735,\n",
       " 0.5601806710374467,\n",
       " 0.5552153308140818,\n",
       " 0.550347865229169,\n",
       " 0.5455767879706798,\n",
       " 0.5409006080266714,\n",
       " 0.5363178323928598,\n",
       " 0.5318269685687048,\n",
       " 0.527426526849375,\n",
       " 0.523115022422093,\n",
       " 0.5188909772761783,\n",
       " 0.5147529219366234,\n",
       " 0.5106993970313228,\n",
       " 0.5067289547021592,\n",
       " 0.5028401598700691,\n",
       " 0.49903159136400926,\n",
       " 0.4953018429234378,\n",
       " 0.4916495240835543,\n",
       " 0.48807326095211667,\n",
       " 0.48457169688619767,\n",
       " 0.48114349307677384,\n",
       " 0.47778732904855803,\n",
       " 0.47450190308201545,\n",
       " 0.47128593256404044,\n",
       " 0.4681381542733183,\n",
       " 0.4650573246059744,\n",
       " 0.46204221974670207,\n",
       " 0.4590916357901731,\n",
       " 0.45620438881718267,\n",
       " 0.45337931492963107,\n",
       " 0.450615270248141,\n",
       " 0.4479111308758057,\n",
       " 0.4452657928313007,\n",
       " 0.4426781719543332,\n",
       " 0.4401472037861743,\n",
       " 0.4376718434278022,\n",
       " 0.4352510653779848,\n",
       " 0.432883863353449,\n",
       " 0.4305692500931106,\n",
       " 0.42830625714818404,\n",
       " 0.4260939346598481,\n",
       " 0.4239313511260039,\n",
       " 0.42181759315854567,\n",
       " 0.41975176523244356,\n",
       " 0.41773298942783565,\n",
       " 0.4157604051662289,\n",
       " 0.4138331689418149,\n",
       " 0.4119504540488254,\n",
       " 0.4101114503057741,\n",
       " 0.40831536377735816,\n",
       " 0.4065614164947283,\n",
       " 0.40484884617477196,\n",
       " 0.4031769059389978,\n",
       " 0.40154486403255896,\n",
       " 0.39995200354389937,\n",
       " 0.3983976221254632,\n",
       " 0.39688103171586736,\n",
       " 0.39540155826389545,\n",
       " 0.39395854145463527,\n",
       " 0.3925513344380502,\n",
       " 0.39117930356024155,\n",
       " 0.3898418280976328,\n",
       " 0.3885382999942771,\n",
       " 0.38726812360246715,\n",
       " 0.38603071542680456,\n",
       " 0.3848255038718606,\n",
       " 0.38365192899354705,\n",
       " 0.3825094422542941,\n",
       " 0.38139750628211655,\n",
       " 0.3803155946336365,\n",
       " 0.37926319156111443,\n",
       " 0.37823979178353034,\n",
       " 0.3772449002617446,\n",
       " 0.37627803197775567,\n",
       " 0.37533871171806593,\n",
       " 0.37442647386115474,\n",
       " 0.37354086216905313,\n",
       " 0.3726814295830046,\n",
       " 0.37184773802319143,\n",
       " 0.3710393581925012,\n",
       " 0.37025586938430066,\n",
       " 0.36949685929418113,\n",
       " 0.3687619238356355,\n",
       " 0.36805066695962246,\n",
       " 0.3673627004779704,\n",
       " 0.3666976438905723,\n",
       " 0.36605512421631686,\n",
       " 0.36543477582770395,\n",
       " 0.3648362402890864,\n",
       " 0.3642591661984799,\n",
       " 0.3637032090328845,\n",
       " 0.36316803099705275,\n",
       " 0.36265330087565,\n",
       " 0.3621586938887394,\n",
       " 0.36168389155053315,\n",
       " 0.361228581531347,\n",
       " 0.3607924575226956,\n",
       " 0.3603752191054666,\n",
       " 0.35997657162111124,\n",
       " 0.35959622604579033,\n",
       " 0.35923389886741364,\n",
       " 0.35888931196551155,\n",
       " 0.3585621924938799,\n",
       " 0.35825227276593646,\n",
       " 0.35795929014273103,\n",
       " 0.357682986923551,\n",
       " 0.35742311023906365,\n",
       " 0.35717941194693975,\n",
       " 0.3569516485299011,\n",
       " 0.35673958099613845,\n",
       " 0.3565429747820445,\n",
       " 0.35636159965720865,\n",
       " 0.3561952296316224,\n",
       " 0.35604364286504314,\n",
       " 0.3559066215784655,\n",
       " 0.35578395196765206,\n",
       " 0.3556754241186742,\n",
       " 0.35558083192541534,\n",
       " 0.3554999730089911,\n",
       " 0.3554326486390395,\n",
       " 0.35537866365683796,\n",
       " 0.3553378264002022,\n",
       " 0.3553099486301254,\n",
       " 0.3552948454591155,\n",
       " 0.3552923352811899,\n",
       " 0.35530223970348684,\n",
       " 0.3553243834794559,\n",
       " 0.35535859444358897,\n",
       " 0.35540470344765407,\n",
       " 0.35546254429839563,\n",
       " 0.3555319536966671,\n",
       " 0.35561277117795875,\n",
       " 0.35570483905429,\n",
       " 0.35580800235743026,\n",
       " 0.35592210878341757,\n",
       " 0.35604700863834426,\n",
       " 0.3561825547853766,\n",
       " 0.3563286025929814,\n",
       " 0.3564850098843275,\n",
       " 0.3566516368878363,\n",
       " 0.35682834618885084,\n",
       " 0.3570150026823995,\n",
       " 0.3572114735270239,\n",
       " 0.3574176280996494,\n",
       " 0.35763333795146934,\n",
       " 0.3578584767648213,\n",
       " 0.3580929203110297,\n",
       " 0.35833654640919194,\n",
       " 0.3585892348858858,\n",
       " 0.35885086753577483,\n",
       " 0.3591213280830911,\n",
       " 0.35940050214397395,\n",
       " 0.3596882771896433,\n",
       " 0.3599845425103896,\n",
       " 0.36028918918035746,\n",
       " 0.3606021100231081,\n",
       " 0.3609231995779381,\n",
       " 0.36125235406693823,\n",
       " 0.3615894713627759,\n",
       " 0.36193445095718085,\n",
       " 0.36228719393011954,\n",
       " 0.3626476029196418,\n",
       " 0.36301558209238305,\n",
       " 0.363391037114706,\n",
       " 0.3637738751244686,\n",
       " 0.36416400470340105,\n",
       " 0.36456133585007944,\n",
       " 0.3649657799534808,\n",
       " 0.36537724976710556,\n",
       " 0.3657956593836557,\n",
       " 0.36622092421025354,\n",
       " 0.3666529609441892,\n",
       " 0.3670916875491857,\n",
       " 0.36753702323216664,\n",
       " 0.3679888884205177,\n",
       " 0.36844720473982795,\n",
       " 0.36891189499210214,\n",
       " 0.36938288313443096,\n",
       " 0.3698600942581093,\n",
       " 0.37034345456819295,\n",
       " 0.3708328913634817,\n",
       " 0.3713283330169216,\n",
       " 0.37182970895641326,\n",
       " 0.37233694964602115,\n",
       " 0.3728499865675695,\n",
       " 0.3733687522026207,\n",
       " 0.37389318001482474,\n",
       " 0.374423204432631,\n",
       " 0.3749587608323563,\n",
       " 0.3754997855215981,\n",
       " 0.3760462157229876,\n",
       " 0.37659798955827334,\n",
       " 0.3771550460327285,\n",
       " 0.37771732501987404,\n",
       " 0.3782847672465115,\n",
       " 0.3788573142780571,\n",
       " 0.3794349085041709,\n",
       " 0.3800174931246756,\n",
       " 0.38060501213575576,\n",
       " 0.3811974103164343,\n",
       " 0.3817946332153165,\n",
       " 0.3823966271375998,\n",
       " 0.38300333913233886,\n",
       " 0.38361471697996413,\n",
       " 0.3842307091800457,\n",
       " 0.38485126493929767,\n",
       " 0.385476334159819,\n",
       " 0.38610586742756325,\n",
       " 0.3867398160010338,\n",
       " 0.38737813180019975,\n",
       " 0.3880207673956261,\n",
       " 0.38866767599781477,\n",
       " 0.389318811446752,\n",
       " 0.38997412820165644,\n",
       " 0.39063358133092446,\n",
       " 0.3912971265022683,\n",
       " 0.3919647199730421,\n",
       " 0.3926363185807533,\n",
       " 0.3933118797337534,\n",
       " 0.3939913614021058,\n",
       " 0.3946747221086259,\n",
       " 0.39536192092009087,\n",
       " 0.39605291743861365,\n",
       " 0.39674767179317955,\n",
       " 0.39744614463134065,\n",
       " 0.39814829711106575,\n",
       " 0.3988540908927406,\n",
       " 0.39956348813131864,\n",
       " 0.40027645146861446,\n",
       " 0.4009929440257415,\n",
       " 0.4017129293956873,\n",
       " 0.4024363716360265,\n",
       " 0.40316323526176573,\n",
       " 0.40389348523831986,\n",
       " 0.4046270869746158,\n",
       " 0.40536400631632175,\n",
       " 0.4061042095391979,\n",
       " 0.406847663342569,\n",
       " 0.40759433484291246,\n",
       " 0.4083441915675623,\n",
       " 0.4090972014485262,\n",
       " 0.409853332816412,\n",
       " 0.4106125543944629,\n",
       " 0.41137483529269864,\n",
       " 0.41214014500215906,\n",
       " 0.4129084533892512,\n",
       " 0.41367973069019415,\n",
       " 0.41445394750556225,\n",
       " 0.41523107479492394,\n",
       " 0.4160110838715732,\n",
       " 0.4167939463973534,\n",
       " 0.41757963437757145,\n",
       " 0.4183681201559983,\n",
       " 0.4191593764099574,\n",
       " 0.41995337614549677,\n",
       " 0.4207500926926443,\n",
       " 0.4215494997007437,\n",
       " 0.4223515711338702,\n",
       " 0.4231562812663253,\n",
       " 0.4239636046782062,\n",
       " 0.4247735162510515,\n",
       " 0.4255859911635597,\n",
       " 0.4264010048873803,\n",
       " 0.42721853318297454,\n",
       " 0.42803855209554686,\n",
       " 0.42886103795104297,\n",
       " 0.42968596735221476,\n",
       " 0.43051331717475144,\n",
       " 0.431343064563473,\n",
       " 0.43217518692858836,\n",
       " 0.4330096619420137,\n",
       " 0.4338464675337515,\n",
       " 0.4346855818883298,\n",
       " 0.43552698344129787,\n",
       " 0.4363706508757807,\n",
       " 0.43721656311908774,\n",
       " 0.438064699339378,\n",
       " 0.4389150389423781,\n",
       " 0.4397675615681535,\n",
       " 0.4406222470879313,\n",
       " 0.4414790756009737,\n",
       " 0.442338027431503,\n",
       " 0.44319908312567285,\n",
       " 0.44406222344859003,\n",
       " 0.44492742938138186,\n",
       " 0.44579468211831125,\n",
       " 0.4466639630639353,\n",
       " 0.44753525383031145,\n",
       " 0.4484085362342434,\n",
       " 0.44928379229457455,\n",
       " 0.45016100422952016,\n",
       " 0.4510401544540419,\n",
       " 0.4519212255772653,\n",
       " 0.45280420039993274,\n",
       " 0.45368906191190056,\n",
       " 0.45457579328967085,\n",
       " 0.4554643778939633,\n",
       " 0.4563547992673235,\n",
       " 0.45724704113176773,\n",
       " 0.4581410873864631,\n",
       " 0.4590369221054448,\n",
       " 0.45993452953536507,\n",
       " 0.46083389409327913,\n",
       " 0.4617350003644617,\n",
       " 0.46263783310025863,\n",
       " 0.46354237721596914,\n",
       " 0.46444861778876123,\n",
       " 0.46535654005561683,\n",
       " 0.4662661294113071,\n",
       " 0.46717737140639987,\n",
       " 0.4680902517452963,\n",
       " 0.4690047562842923,\n",
       " 0.4699208710296759,\n",
       " 0.47083858213584506,\n",
       " 0.4717578759034591,\n",
       " 0.47267873877761285,\n",
       " 0.4736011573460398,\n",
       " 0.474525118337341,\n",
       " 0.4754506086192389,\n",
       " 0.4763776151968573,\n",
       " 0.477306125211026,\n",
       " 0.478236125936609,\n",
       " 0.4791676047808581,\n",
       " 0.48010054928178963,\n",
       " 0.48103494710658296,\n",
       " 0.48197078605000454,\n",
       " 0.48290805403285303,\n",
       " 0.4838467391004248,\n",
       " 0.4847868294210046,\n",
       " 0.48572831328437455,\n",
       " 0.48667117910034674,\n",
       " 0.4876154153973137,\n",
       " 0.4885610108208222,\n",
       " 0.48950795413216364,\n",
       " 0.49045623420698825,\n",
       " 0.49140584003393406,\n",
       " 0.49235676071327966,\n",
       " 0.4933089854556105,\n",
       " 0.49426250358050955,\n",
       " 0.49521730451526114,\n",
       " 0.4961733777935728,\n",
       " 0.49713071305432077,\n",
       " 0.4980893000403042,\n",
       " 0.49904912859702305,\n",
       " 0.5000101886714683,\n",
       " 0.5009724703109327,\n",
       " 0.5019359636618329,\n",
       " 0.5029006589685526,\n",
       " 0.5038665465722983,\n",
       " 0.5048336169099695,\n",
       " 0.5058018605130497,\n",
       " 0.506771268006503,\n",
       " 0.5077418301076969,\n",
       " 0.5087135376253288,\n",
       " 0.509686381458375,\n",
       " 0.5106603525950479,\n",
       " 0.511635442111773,\n",
       " 0.5126116411721715,\n",
       " 0.5135889410260648,\n",
       " 0.5145673330084877,\n",
       " 0.515546808538713,\n",
       " 0.5165273591192945,\n",
       " 0.5175089763351182,\n",
       " 0.5184916518524655,\n",
       " 0.5194753774180928,\n",
       " 0.5204601448583186,\n",
       " 0.5214459460781276,\n",
       " 0.522432773060279,\n",
       " 0.5234206178644348,\n",
       " 0.5244094726262936,\n",
       " 0.5253993295567372,\n",
       " 0.5263901809409895,\n",
       " 0.5273820191377859,\n",
       " 0.5283748365785493,\n",
       " 0.5293686257665843,\n",
       " 0.5303633792762761,\n",
       " 0.5313590897522982,\n",
       " 0.5323557499088397,\n",
       " 0.5333533525288278,\n",
       " 0.5343518904631754,\n",
       " 0.5353513566300272,\n",
       " 0.5363517440140186,\n",
       " 0.5373530456655503,\n",
       " 0.5383552547000572,\n",
       " 0.5393583642973043,\n",
       " 0.5403623677006806,\n",
       " 0.5413672582164998,\n",
       " 0.5423730292133209,\n",
       " 0.5433796741212652,\n",
       " 0.5443871864313491,\n",
       " 0.5453955596948243,\n",
       " 0.5464047875225252,\n",
       " 0.5474148635842216,\n",
       " 0.5484257816079853,\n",
       " 0.5494375353795613,\n",
       " 0.5504501187417442,\n",
       " 0.5514635255937689,\n",
       " 0.5524777498907018,\n",
       " 0.5534927856428443,\n",
       " 0.5545086269151421,\n",
       " 0.5555252678266013,\n",
       " 0.5565427025497097,\n",
       " 0.5575609253098734,\n",
       " 0.5585799303848469,\n",
       " 0.5595997121041831,\n",
       " 0.560620264848681,\n",
       " 0.5616415830498452,\n",
       " 0.5626636611893506,\n",
       " 0.5636864937985143,\n",
       " 0.564710075457765,\n",
       " 0.5657344007961383,\n",
       " 0.5667594644907601,\n",
       " 0.5677852612663401,\n",
       " 0.5688117858946768,\n",
       " 0.5698390331941704,\n",
       " 0.5708669980293243,\n",
       " 0.571895675310275,\n",
       " 0.572925059992314,\n",
       " 0.5739551470754175,\n",
       " 0.5749859316037855,\n",
       " 0.5760174086653814,\n",
       " 0.5770495733914773,\n",
       " 0.5780824209562163,\n",
       " 0.5791159465761551,\n",
       " 0.580150145509845,\n",
       " 0.581185013057382,\n",
       " 0.5822205445599978,\n",
       " 0.5832567353996226,\n",
       " 0.5842935809984805,\n",
       " 0.5853310768186659,\n",
       " 0.5863692183617548,\n",
       " 0.5874080011683785,\n",
       " 0.5884474208178446,\n",
       " 0.58948747292774,\n",
       " 0.5905281531535349,\n",
       " 0.5915694571882056,\n",
       " 0.5926113807618589,\n",
       " 0.5936539196413456,\n",
       " 0.5946970696298989,\n",
       " 0.5957408265667622,\n",
       " 0.5967851863268344,\n",
       " 0.597830144820304,\n",
       " 0.5988756979922936,\n",
       " 0.5999218418225232,\n",
       " 0.6009685723249483,\n",
       " 0.6020158855474254,\n",
       " 0.6030637775713746,\n",
       " 0.6041122445114447,\n",
       " 0.6051612825151794,\n",
       " 0.6062108877626946,\n",
       " 0.6072610564663533,\n",
       " 0.6083117848704434,\n",
       " 0.6093630692508649,\n",
       " 0.6104149059148264,\n",
       " 0.611467291200517,\n",
       " 0.6125202214768138,\n",
       " 0.613573693142983,\n",
       " 0.6146277026283699,\n",
       " 0.6156822463921124,\n",
       " 0.6167373209228486,\n",
       " 0.6177929227384231,\n",
       " 0.6188490483856041,\n",
       " 0.6199056944398073,\n",
       " 0.6209628575048086,\n",
       " 0.6220205342124666,\n",
       " 0.6230787212224609,\n",
       " 0.6241374152220114,\n",
       " 0.6251966129256159,\n",
       " 0.6262563110747894,\n",
       " 0.6273165064377891,\n",
       " 0.6283771958093766,\n",
       " 0.6294383760105475,\n",
       " 0.6305000438882894,\n",
       " 0.6315621963153155,\n",
       " 0.6326248301898352,\n",
       " 0.6336879424353032,\n",
       " 0.6347515300001783,\n",
       " 0.6358155898576746,\n",
       " 0.6368801190055481,\n",
       " 0.6379451144658373,\n",
       " 0.6390105732846479,\n",
       " 0.6400764925319179,\n",
       " 0.6411428693011924,\n",
       " 0.6422097007094056,\n",
       " 0.6432769838966436,\n",
       " 0.6443447160259412,\n",
       " 0.6454128942830568,\n",
       " 0.6464815158762637,\n",
       " 0.6475505780361349,\n",
       " 0.6486200780153303,\n",
       " 0.6496900130883857,\n",
       " 0.6507603805515286,\n",
       " 0.6518311777224467,\n",
       " 0.6529024019401068,\n",
       " 0.6539740505645499,\n",
       " 0.6550461209766831,\n",
       " 0.6561186105781125,\n",
       " 0.6571915167909174,\n",
       " 0.6582648370574927,\n",
       " 0.6593385688403265,\n",
       " 0.6604127096218383,\n",
       " 0.6614872569041912,\n",
       " 0.6625622082091042,\n",
       " 0.6636375610776643,\n",
       " 0.6647133130701537,\n",
       " 0.6657894617658937,\n",
       " 0.6668660047630299,\n",
       " 0.6679429396783928,\n",
       " 0.6690202641472998,\n",
       " 0.6700979758234226,\n",
       " 0.6711760723785628,\n",
       " 0.6722545515025515,\n",
       " 0.6733334109030322,\n",
       " 0.6744126483053237,\n",
       " 0.6754922614522477,\n",
       " 0.6765722481039858,\n",
       " 0.6776526060379041,\n",
       " 0.6787333330484099,\n",
       " 0.6798144269467808,\n",
       " 0.6808958855610425,\n",
       " 0.6819777067357666,\n",
       " 0.6830598883319899,\n",
       " 0.6841424282269978,\n",
       " 0.6852253243142213,\n",
       " 0.6863085745030815,\n",
       " 0.6873921767188309,\n",
       " 0.6884761289024421,\n",
       " 0.6895604290104284,\n",
       " 0.6906450750147447,\n",
       " 0.6917300649026156,\n",
       " 0.6928153966764188,\n",
       " 0.6939010683535359,\n",
       " 0.6949870779662332,\n",
       " 0.6960734235615362,\n",
       " 0.6971601032010551,\n",
       " 0.698247114960916,\n",
       " 0.6993344569315925,\n",
       " 0.700422127217774,\n",
       " 0.7015101239382617,\n",
       " 0.7025984452258429,\n",
       " 0.7036870892271793,\n",
       " 0.7047760541026195,\n",
       " 0.7058653380261616,\n",
       " 0.706954939185294,\n",
       " 0.708044855780853,\n",
       " 0.7091350860269859,\n",
       " 0.7102256281509239,\n",
       " 0.7113164803929786,\n",
       " 0.7124076410063301,\n",
       " 0.71349910825698,\n",
       " 0.7145908804236215,\n",
       " 0.7156829557975358,\n",
       " 0.7167753326824419,\n",
       " 0.7178680093944576,\n",
       " 0.7189609842619232,\n",
       " 0.7200542556253666,\n",
       " 0.7211478218373009,\n",
       " 0.7222416812622416,\n",
       " 0.7233358322764992,\n",
       " 0.7244302732681235,\n",
       " 0.7255250026368224,\n",
       " 0.7266200187938127,\n",
       " 0.7277153201617514,\n",
       " 0.7288109051746372,\n",
       " 0.7299067722777126,\n",
       " 0.7310029199273362,\n",
       " 0.7320993465909447,\n",
       " 0.7331960507469232,\n",
       " 0.7342930308845218,\n",
       " 0.7353902855037205,\n",
       " 0.7364878131152413,\n",
       " 0.7375856122403277,\n",
       " 0.7386836814107527,\n",
       " 0.7397820191687213,\n",
       " 0.7408806240666946,\n",
       " 0.7419794946674161,\n",
       " 0.7430786295437425,\n",
       " 0.7441780272786129,\n",
       " 0.7452776864649684,\n",
       " 0.7463776057055821,\n",
       " 0.7474777836130624,\n",
       " 0.7485782188097603,\n",
       " 0.7496789099276467,\n",
       " 0.7507798556082633,\n",
       " 0.7518810545026371,\n",
       " 0.7529825052712166,\n",
       " 0.7540842065837852,\n",
       " 0.7551861571193187,\n",
       " 0.7562883555659936,\n",
       " 0.7573908006211332,\n",
       " 0.7584934909910079,\n",
       " 0.7595964253908635,\n",
       " 0.7606996025448349,\n",
       " 0.7618030211858009,\n",
       " 0.7629066800554457,\n",
       " 0.7640105779040565,\n",
       " 0.7651147134904992,\n",
       " 0.7662190855821835,\n",
       " 0.7673236929548971,\n",
       " 0.76842853439292,\n",
       " 0.769533608688684,\n",
       " 0.7706389146429864,\n",
       " 0.7717444510647097,\n",
       " 0.7728502167709097,\n",
       " 0.7739562105865866,\n",
       " 0.7750624313448155,\n",
       " 0.7761688778864564,\n",
       " 0.7772755490603612,\n",
       " 0.7783824437230091,\n",
       " 0.7794895607387082,\n",
       " 0.7805968989793247,\n",
       " 0.7817044573243925,\n",
       " 0.7828122346609291,\n",
       " 0.7839202298834583,\n",
       " 0.7850284418938833,\n",
       " 0.7861368696014026,\n",
       " 0.7872455119226487,\n",
       " 0.7883543677813077,\n",
       " 0.7894634361084132,\n",
       " 0.7905727158419923,\n",
       " 0.791682205927082,\n",
       " 0.7927919053159733,\n",
       " 0.7939018129675739,\n",
       " 0.795011927847913,\n",
       " 0.7961222489297648,\n",
       " 0.797232775192737,\n",
       " 0.7983435056231172,\n",
       " 0.7994544392138392,\n",
       " 0.8005655749645071,\n",
       " 0.8016769118813296,\n",
       " 0.8027884489769495,\n",
       " 0.8039001852704802,\n",
       " 0.8050121197875566,\n",
       " 0.8061242515600195,\n",
       " 0.8072365796261263,\n",
       " 0.8083491030303814,\n",
       " 0.8094618208235171,\n",
       " 0.8105747320623917,\n",
       " 0.8116878358099395,\n",
       " 0.8128011311353112,\n",
       " 0.813914617113568,\n",
       " 0.8150282928256786,\n",
       " 0.8161421573587168,\n",
       " 0.8172562098055582,\n",
       " 0.8183704492648628,\n",
       " 0.819484874841136,\n",
       " 0.8205994856445961,\n",
       " 0.8217142807911908,\n",
       " 0.8228292594025355,\n",
       " 0.8239444206058855,\n",
       " 0.8250597635339393,\n",
       " 0.8261752873249959,\n",
       " 0.8272909911229592,\n",
       " 0.8284068740768814,\n",
       " 0.8295229353415525,\n",
       " 0.8306391740768967,\n",
       " 0.8317555894482114,\n",
       " 0.8328721806260804,\n",
       " 0.8339889467863602,\n",
       " 0.8351058871101269,\n",
       " 0.8362230007833676,\n",
       " 0.8373402869975453,\n",
       " 0.838457744948906,\n",
       " 0.8395753738389359,\n",
       " 0.8406931728740475,\n",
       " 0.8418111412654721,\n",
       " 0.8429292782297491,\n",
       " 0.8440475829878628,\n",
       " 0.8451660547658918,\n",
       " 0.8462846927947102,\n",
       " 0.8474034963100207,\n",
       " 0.8485224645520223,\n",
       " 0.849641596765952,\n",
       " 0.8507608922014114,\n",
       " 0.8518803501128916,\n",
       " 0.8529999697594292,\n",
       " 0.8541197504042927,\n",
       " 0.8552396913158582,\n",
       " 0.8563597917666042,\n",
       " 0.8574800510335864,\n",
       " 0.8586004683982136,\n",
       " 0.8597210431464567,\n",
       " 0.8608417745684804,\n",
       " 0.8619626619588739,\n",
       " 0.8630837046165216,\n",
       " 0.8642049018444588,\n",
       " 0.8653262529502687,\n",
       " 0.8664477572453768,\n",
       " 0.8675694140456165,\n",
       " 0.8686912226707991,\n",
       " 0.8698131824450686,\n",
       " 0.8709352926964357,\n",
       " 0.8720575527569957,\n",
       " 0.873179961963121,\n",
       " 0.8743025196547926,\n",
       " 0.8754252251761058,\n",
       " 0.8765480778753586,\n",
       " 0.8776710771042244,\n",
       " 0.8787942222187576,\n",
       " 0.8799175125786549,\n",
       " 0.881040947547398,\n",
       " 0.8821645264923307,\n",
       " 0.8832882487843102,\n",
       " 0.8844121137983594,\n",
       " 0.8855361209129592,\n",
       " 0.8866602695105434,\n",
       " 0.8877845589764868,\n",
       " 0.8889089887007867,\n",
       " 0.8900335580762101,\n",
       " 0.8911582664996575,\n",
       " 0.8922831133711946,\n",
       " 0.8934080980946575,\n",
       " 0.8945332200773041,\n",
       " 0.8956584787298508,\n",
       " 0.8967838734666559,\n",
       " 0.8979094037050027,\n",
       " 0.8990350688663596,\n",
       " 0.9001608683746659,\n",
       " 0.9012868016579985,\n",
       " 0.9024128681471962,\n",
       " 0.9035390672768006,\n",
       " 0.9046653984845242,\n",
       " 0.9057918612114108,\n",
       " 0.9069184549011287,\n",
       " 0.9080451790016089,\n",
       " 0.9091720329631278,\n",
       " 0.9102990162395163,\n",
       " 0.911426128287684,\n",
       " 0.912553368567485,\n",
       " 0.9136807365420792,\n",
       " 0.9148082316777575,\n",
       " 0.9159358534436681,\n",
       " 0.9170636013120242,\n",
       " 0.9181914747584946,\n",
       " 0.919319473260623,\n",
       " 0.9204475963006103,\n",
       " 0.9215758433620114,\n",
       " 0.9227042139320537,\n",
       " 0.9238327075008856,\n",
       " 0.9249613235613962,\n",
       " 0.9260900616093372,\n",
       " 0.9272189211437373,\n",
       " 0.9283479016656462,\n",
       " 0.9294770026796495,\n",
       " 0.93060622369248,\n",
       " 0.9317355642144349,\n",
       " 0.9328650237577563,\n",
       " 0.9339946018379536,\n",
       " 0.9351242979731775,\n",
       " 0.9362541116840072,\n",
       " 0.937384042493857,\n",
       " 0.9385140899290992,\n",
       " 0.9396442535182654,\n",
       " 0.9407745327926128,\n",
       " 0.9419049272864612,\n",
       " 0.9430354365359993,\n",
       " 0.9441660600800674,\n",
       " 0.9452967974610152,\n",
       " 0.9464276482229679,\n",
       " 0.9475586119119669,\n",
       " 0.9486896880776454,\n",
       " 0.9498208762718868,\n",
       " 0.9509521760486587,\n",
       " 0.9520835869644114,\n",
       " 0.9532151085782715,\n",
       " 0.9543467404515203,\n",
       " 0.9554784821487122,\n",
       " 0.9566103332355161,\n",
       " 0.9577422932801002,\n",
       " 0.9588743618543031,\n",
       " 0.9600065385309815,\n",
       " 0.9611388228856447,\n",
       " 0.9622712144965112,\n",
       " 0.9634037129435423,\n",
       " 0.9645363178087201,\n",
       " 0.9656690286780223,\n",
       " 0.9668018451368856,\n",
       " 0.9679347667759641,\n",
       " 0.9690677931854812,\n",
       " 0.9702009239596894,\n",
       " 0.971334158693933,\n",
       " 0.9724674969868855,\n",
       " 0.9736009384380994,\n",
       " 0.9747344826494295,\n",
       " 0.9758681292262039,\n",
       " 0.977001877774521,\n",
       " 0.9781357279023011,\n",
       " 0.9792696792209398,\n",
       " 0.9804037313428969,\n",
       " 0.9815378838824048,\n",
       " 0.9826721364579202,\n",
       " 0.9838064886868929,\n",
       " 0.9849409401900764,\n",
       " 0.9860754905913575,\n",
       " 0.9872101395146745,\n",
       " 0.9883448865869846,\n",
       " 0.98947973143749,\n",
       " 0.9906146736969226,\n",
       " 0.9917497129979568,\n",
       " 0.9928848489747907,\n",
       " 0.9940200812638688,\n",
       " 0.9951554095041615,\n",
       " 0.9962908333355612,\n",
       " 0.9974263524005461,\n",
       " 0.9985619663428735,\n",
       " 0.9996976748083404,\n",
       " 1.000833477445396,\n",
       " 1.0019693739025506,\n",
       " 1.003105363832561,\n",
       " 1.004241446887123,\n",
       " 1.005377622721415,\n",
       " 1.00651389099293,\n",
       " 1.0076502513593988,\n",
       " 1.008786703482304,\n",
       " 1.0099232470222579,\n",
       " 1.0110598816438516,\n",
       " 1.0121966070116901,\n",
       " 1.0133334227952466,\n",
       " 1.014470328660134,\n",
       " 1.0156073242787058,\n",
       " 1.0167444093236129,\n",
       " 1.0178815834677541,\n",
       " 1.01901884638701,\n",
       " 1.0201561977591147,\n",
       " 1.0212936372625068,\n",
       " 1.0224311645769042,\n",
       " 1.0235687793852826,\n",
       " 1.0247064813720412,\n",
       " 1.0258442702212063,\n",
       " 1.0269821456199983,\n",
       " 1.0281201072573631,\n",
       " 1.0292581548238278,\n",
       " 1.0303962880088549,\n",
       " 1.0315345065078316,\n",
       " 1.0326728100134817,\n",
       " 1.0338111982232603,\n",
       " 1.0349496708360706,\n",
       " 1.0360882275488092,\n",
       " 1.0372268680625887,\n",
       " 1.038365592081129,\n",
       " 1.0395043993061766,\n",
       " 1.0406432894438498,\n",
       " 1.041782262201072,\n",
       " 1.0429213172857175,\n",
       " 1.044060454406964,\n",
       " 1.0451996732764555,\n",
       " 1.046338973605124,\n",
       " 1.0474783551086269,\n",
       " 1.0486178175020398,\n",
       " 1.0497573604998969,\n",
       " 1.0508969838216597,\n",
       " 1.0520366871856908,\n",
       " 1.0531764703150024,\n",
       " 1.0543163329295582,\n",
       " 1.0554562747533227,\n",
       " 1.056596295512063,\n",
       " 1.0577363949300715,\n",
       " 1.0588765727353469,\n",
       " 1.0600168286576725,\n",
       " 1.0611571624286114,\n",
       " 1.0622975737755906,\n",
       " 1.0634380624340516,\n",
       " 1.0645786281379432,\n",
       " 1.065719270622238,\n",
       " 1.0668599896235886,\n",
       " 1.0680007848779858,\n",
       " 1.069141656126112,\n",
       " 1.0702826031091799,\n",
       " 1.071423625567966,\n",
       " 1.072564723245329,\n",
       " 1.073705895883399,\n",
       " 1.0748471432312539,\n",
       " 1.0759884650339684,\n",
       " 1.0771298610357982,\n",
       " 1.0782713309900374,\n",
       " 1.0794128746480725,\n",
       " 1.0805544917541592,\n",
       " 1.0816961820677935,\n",
       " 1.0828379453368797,\n",
       " 1.0839797813235639,\n",
       " 1.0851216897753433,\n",
       " 1.086263670456102,\n",
       " 1.0874057231195793,\n",
       " 1.088547847528892,\n",
       " 1.0896900434408199,\n",
       " 1.0908323106189877,\n",
       " 1.0919746488269766,\n",
       " 1.0931170578270817,\n",
       " 1.094259537383947,\n",
       " 1.0954020872654735,\n",
       " 1.0965447072373167,\n",
       " 1.0976873970687617,\n",
       " 1.0988301565291096,\n",
       " 1.0999729853872655,\n",
       " 1.1011158834150685,\n",
       " 1.1022588503891424,\n",
       " 1.1034018860750385,\n",
       " 1.1045449902543416,\n",
       " 1.1056881626998893,\n",
       " 1.1068314031871591,\n",
       " 1.107974711501013,\n",
       " 1.1091180874075515,\n",
       " 1.1102615306979373,\n",
       " 1.1114050411466116,\n",
       " 1.112548618536206,\n",
       " 1.113692262655515,\n",
       " 1.114835973280214,\n",
       " 1.1159797501955586,\n",
       " 1.1171235931924968,\n",
       " 1.1182675020568655,\n",
       " 1.1194114765713854,\n",
       " 1.1205555165274288,\n",
       " 1.1216996217189918,\n",
       " 1.1228437919290555,\n",
       " 1.1239880269564293,\n",
       " 1.1251323265831172,\n",
       " 1.126276690614454,\n",
       " 1.1274211188361147,\n",
       " 1.128565611050414,\n",
       " 1.1297101670473881,\n",
       " 1.1308547866300451,\n",
       " 1.1319994695896696,\n",
       " 1.1331442157267122,\n",
       " 1.1342890248425743,\n",
       " 1.1354338967391358,\n",
       " 1.136578831218815,\n",
       " 1.137723828078254,\n",
       " 1.1388688871264183,\n",
       " 1.1400140081672037,\n",
       " 1.1411591909992957,\n",
       " 1.1423044354323841,\n",
       " 1.1434497412790527,\n",
       " ...]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.learning_curve(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harut\\AppData\\Local\\Temp\\ipykernel_14688\\3341466286.py:27: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -np.mean(y*np.log(self.sigmoid(X,w)) + (1-y)*np.log(1- self.sigmoid(X,w)))\n",
      "C:\\Users\\harut\\AppData\\Local\\Temp\\ipykernel_14688\\3341466286.py:27: RuntimeWarning: invalid value encountered in multiply\n",
      "  cost = -np.mean(y*np.log(self.sigmoid(X,w)) + (1-y)*np.log(1- self.sigmoid(X,w)))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(k.learning_curve(X_train,y_train))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8357142857142857"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(k.predict(X_train) == y_train)/len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "\n",
    "## Digits classification\n",
    "**[30 points]**\n",
    "\n",
    "*An exploration of regularization, imbalanced classes, ROC and PR curves*\n",
    "\n",
    "The goal of this exercise is to apply your supervised learning skills on a very different dataset: in this case, image data; MNIST: a collection of images of handwritten digits. Your goal is to train a classifier that is able to distinguish the number \"3\" from all possible numbers and to do so as accurately as possible. You will first explore your data (this should always be your starting point to gain domain knowledge about the problem.). Since the feature space in this problem is 784-dimensional, overfitting is possible. To avoid overfitting you will investigate the impact of regularization on generalization performance (test accuracy) and compare regularized and unregularized logistic regression model test error against other classification techniques such as linear discriminant analysis and random forests and draw conclusions about the best-performing model.\n",
    "\n",
    "Start by loading your dataset from the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) of handwritten digits, using the code provided below. MNIST has a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "Your goal is to classify whether or not an example digit is a 3. Your binary classifier should predict $y=1$ if the digit is a 3, and $y=0$ otherwise. Create your dataset by transforming your labels into a binary format (3's are class 1, and all other digits are class 0). \n",
    "\n",
    "**(a)** Plot 10 examples of each class (i.e. class $y=0$, which are not 3's and class $y=1$ which are 3's), from the training dataset.\n",
    "- Note that the data are composed of samples of length 784. These represent 28 x 28 images, but have been reshaped for storage convenience. To plot digit examples, you'll need to reshape the data to be 28 x 28 (which can be done with numpy `reshape`).\n",
    "\n",
    "**(b)** How many examples are present in each class? Show a plot of samples by class (bar plot). What fraction of samples are positive? What issues might this cause?\n",
    "\n",
    "**(c)** Using a logistic regression classifier, apply lasso regularization and retrain the model and evaluate its performance over a range of values on the regularization coefficient. You can implement this using the [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) module and activating the 'l1' penalty; the parameter $C$ is the inverse of the regularization strength. Vary the value of C logarithmically from $10^{-4}$ to $10^4$ (and make your x-axes logarithmic in scale) and evaluate it at 20 different values of C. As you vary the regularization coefficient, Plot \n",
    "- The number of model parameters that are estimated to be nonzero (in the logistic regression model, one attribute is `coef_`, which gives you access to the model parameters for a trained model)\n",
    "- The cross entropy loss (which can be evaluated with the Scikit Learn `log_loss` function)\n",
    "- Area under the ROC curve (AUC)\n",
    "- The $F_1$-score (assuming a threshold of 0.5 on the predicted confidence scores, that is, scores above 0.5 are predicted as Class 1, otherwise Class 0). Scikit Learn also has a `f1_score` function which may be useful.\n",
    "- Which value of C seems best for this problem? Please select the closest power of 10. You will use this in the next part of this exercise.\n",
    "\n",
    "**(d)** Train and test a (1) logistic regression classifier with minimal regularization (using the Scikit Learn package, set penalty='l1', C=1e100 to approximate this), (2) a logistic regression classifier with the best value of the regularization parameter from the last section, (3) a Linear Discriminant Analysis (LDA) Classifier, and (4) a Random Forest (RF) classifier (using default parameters for the LDA and RF classifiers). \n",
    "- Compare your classifiers' performance using ROC and Precision Recall (PR) curves. \n",
    "- Plot the line that represents randomly guessing the class (50% of the time a \"3\", 50% not a \"3\"). You SHOULD NOT actually create random guesses. Instead you should think through the theory behind how ROC and PR curves work and plot the appropriate lines. It's a good practice to include these in ROC and PR curve plots as a reference point.\n",
    "- For PR curves, an excellent resource on how to correctly plot them can be found [here](https://classeval.wordpress.com/introduction/introduction-to-the-precision-recall-plot/) (ignore the section on \"non-linear interpolation between two points\"). This describes how a random classifier is represented in PR curves and demonstrates that it should provide a lower bound on performance.\n",
    "- When training your logistic regression model, it's recommended that you use solver=\"liblinear\"; otherwise your results may not converge\n",
    "- Describe the performance of the classifiers you compared. Did the regularization of the logistic regression model make much difference here? Which classifier you would select for application to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST Data\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Set this to True to download the data for the first time and False after the first time \n",
    "#   so that you just load the data locally instead\n",
    "download_data = True\n",
    "\n",
    "if download_data:\n",
    "    # Load data from https://www.openml.org/d/554\n",
    "    X, y = fetch_openml('mnist_784', return_X_y=True, as_frame=False)\n",
    "    \n",
    "    # Adjust the labels to be '1' if y==3, and '0' otherwise\n",
    "    y[y!='3'] = 0\n",
    "    y[y=='3'] = 1\n",
    "    y = y.astype('int')\n",
    "    \n",
    "    # Divide the data intro a training and test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/7, random_state=88)\n",
    "    \n",
    "    file = open('tmpdata', 'wb')\n",
    "    pickle.dump((X_train, X_test, y_train, y_test), file)\n",
    "    file.close()\n",
    "else:\n",
    "    file = open('tmpdata', 'rb')\n",
    "    X_train, X_test, y_train, y_test = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "722px",
    "left": "1550px",
    "right": "20px",
    "top": "121px",
    "width": "353px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
